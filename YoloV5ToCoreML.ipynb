{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/YOLOv5/blob/main/YoloV5ToCoreML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eZMpJbhY9-o-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79526660-afca-4ef5-984f-2329652559ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 14596, done.\u001b[K\n",
            "remote: Counting objects: 100% (150/150), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 14596 (delta 95), reused 99 (delta 52), pack-reused 14446\u001b[K\n",
            "Receiving objects: 100% (14596/14596), 13.50 MiB | 18.83 MiB/s, done.\n",
            "Resolving deltas: 100% (10068/10068), done.\n",
            "/content/yolov5\n"
          ]
        }
      ],
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "%cd yolov5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install coremltools"
      ],
      "metadata": {
        "id": "AXEiB45b9_Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93382636-7541-469e-948d-88fb4dc617a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting coremltools\n",
            "  Downloading coremltools-6.0-cp37-none-manylinux1_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from coremltools) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from coremltools) (1.21.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from coremltools) (1.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from coremltools) (21.3)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from coremltools) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.1.0->coremltools) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->coremltools) (3.0.9)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->coremltools) (1.2.1)\n",
            "Installing collected packages: coremltools\n",
            "Successfully installed coremltools-6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8f0QslFEsSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you use custom traned model, you can change the class labels to your own classes.You can specify as many classes as you like.\n",
        "classLabels = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
        "numberOfClassLabels = len(classLabels)\n",
        "outputSize = numberOfClassLabels + 5"
      ],
      "metadata": {
        "id": "AKkDtPvrD5qr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\\\n",
        "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
        "\n",
        "import argparse\n",
        "import contextlib\n",
        "import json\n",
        "import os\n",
        "import platform\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "FILE = Path(__file__).resolve()\n",
        "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
        "if platform.system() != 'Windows':\n",
        "    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel\n",
        "from utils.dataloaders import LoadImages\n",
        "from utils.general import (LOGGER, Profile, check_dataset, check_img_size, check_requirements, check_version,\n",
        "                           check_yaml, colorstr, file_size, get_default_args, print_args, url2file, yaml_save)\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "\n",
        "MACOS = platform.system() == 'Darwin'  # macOS environment\n",
        "\n",
        "\n",
        "def export_formats():\n",
        "    # YOLOv5 export formats\n",
        "    x = [\n",
        "        ['PyTorch', '-', '.pt', True, True],\n",
        "        ['TorchScript', 'torchscript', '.torchscript', True, True],\n",
        "        ['ONNX', 'onnx', '.onnx', True, True],\n",
        "        ['OpenVINO', 'openvino', '_openvino_model', True, False],\n",
        "        ['TensorRT', 'engine', '.engine', False, True],\n",
        "        ['CoreML', 'coreml', '.mlmodel', True, False],\n",
        "        ['TensorFlow SavedModel', 'saved_model', '_saved_model', True, True],\n",
        "        ['TensorFlow GraphDef', 'pb', '.pb', True, True],\n",
        "        ['TensorFlow Lite', 'tflite', '.tflite', True, False],\n",
        "        ['TensorFlow Edge TPU', 'edgetpu', '_edgetpu.tflite', False, False],\n",
        "        ['TensorFlow.js', 'tfjs', '_web_model', False, False],\n",
        "        ['PaddlePaddle', 'paddle', '_paddle_model', True, True],]\n",
        "    return pd.DataFrame(x, columns=['Format', 'Argument', 'Suffix', 'CPU', 'GPU'])\n",
        "\n",
        "\n",
        "def try_export(inner_func):\n",
        "    # YOLOv5 export decorator, i..e @try_export\n",
        "    inner_args = get_default_args(inner_func)\n",
        "\n",
        "    def outer_func(*args, **kwargs):\n",
        "        prefix = inner_args['prefix']\n",
        "        try:\n",
        "            with Profile() as dt:\n",
        "                f, model = inner_func(*args, **kwargs)\n",
        "            return f, model\n",
        "        except Exception as e:\n",
        "            return None, None\n",
        "\n",
        "    return outer_func\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_torchscript(model, im, file, optimize, prefix=colorstr('TorchScript:')):\n",
        "    # YOLOv5 TorchScript model export\n",
        "    f = file.with_suffix('.torchscript')\n",
        "\n",
        "    ts = torch.jit.trace(model, im, strict=False)\n",
        "    d = {\"shape\": im.shape, \"stride\": int(max(model.stride)), \"names\": model.names}\n",
        "    extra_files = {'config.txt': json.dumps(d)}  # torch._C.ExtraFilesMap()\n",
        "    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n",
        "        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n",
        "    else:\n",
        "        ts.save(str(f), _extra_files=extra_files)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr('ONNX:')):\n",
        "    # YOLOv5 ONNX export\n",
        "    check_requirements('onnx')\n",
        "    import onnx\n",
        "\n",
        "    f = file.with_suffix('.onnx')\n",
        "\n",
        "    output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output0']\n",
        "    if dynamic:\n",
        "        dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)\n",
        "        if isinstance(model, SegmentationModel):\n",
        "            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n",
        "            dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)\n",
        "        elif isinstance(model, DetectionModel):\n",
        "            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model.cpu() if dynamic else model,  # --dynamic only compatible with cpu\n",
        "        im.cpu() if dynamic else im,\n",
        "        f,\n",
        "        verbose=False,\n",
        "        opset_version=opset,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['images'],\n",
        "        output_names=output_names,\n",
        "        dynamic_axes=dynamic or None)\n",
        "\n",
        "    # Checks\n",
        "    model_onnx = onnx.load(f)  # load onnx model\n",
        "    onnx.checker.check_model(model_onnx)  # check onnx model\n",
        "\n",
        "    # Metadata\n",
        "    d = {'stride': int(max(model.stride)), 'names': model.names}\n",
        "    for k, v in d.items():\n",
        "        meta = model_onnx.metadata_props.add()\n",
        "        meta.key, meta.value = k, str(v)\n",
        "    onnx.save(model_onnx, f)\n",
        "\n",
        "    # Simplify\n",
        "    if simplify:\n",
        "        try:\n",
        "            cuda = torch.cuda.is_available()\n",
        "            check_requirements(('onnxruntime-gpu' if cuda else 'onnxruntime', 'onnx-simplifier>=0.4.1'))\n",
        "            import onnxsim\n",
        "\n",
        "            model_onnx, check = onnxsim.simplify(model_onnx)\n",
        "            assert check, 'assert check failed'\n",
        "            onnx.save(model_onnx, f)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    return f, model_onnx\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_openvino(file, metadata, half, prefix=colorstr('OpenVINO:')):\n",
        "    # YOLOv5 OpenVINO export\n",
        "    check_requirements('openvino-dev')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n",
        "    import openvino.inference_engine as ie\n",
        "\n",
        "    f = str(file).replace('.pt', f'_openvino_model{os.sep}')\n",
        "\n",
        "    cmd = f\"mo --input_model {file.with_suffix('.onnx')} --output_dir {f} --data_type {'FP16' if half else 'FP32'}\"\n",
        "    subprocess.run(cmd.split(), check=True, env=os.environ)  # export\n",
        "    yaml_save(Path(f) / file.with_suffix('.yaml').name, metadata)  # add metadata.yaml\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_paddle(model, im, file, metadata, prefix=colorstr('PaddlePaddle:')):\n",
        "    # YOLOv5 Paddle export\n",
        "    check_requirements(('paddlepaddle', 'x2paddle'))\n",
        "    import x2paddle\n",
        "    from x2paddle.convert import pytorch2paddle\n",
        "\n",
        "    f = str(file).replace('.pt', f'_paddle_model{os.sep}')\n",
        "\n",
        "    pytorch2paddle(module=model, save_dir=f, jit_type='trace', input_examples=[im])  # export\n",
        "    yaml_save(Path(f) / file.with_suffix('.yaml').name, metadata)  # add metadata.yaml\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_coreml(model, im, file, int8, half, prefix=colorstr('CoreML:')):\n",
        "    # YOLOv5 CoreML export\n",
        "    check_requirements('coremltools')\n",
        "    import coremltools as ct\n",
        "\n",
        "    f = file.with_suffix('.mlmodel')\n",
        "\n",
        "    ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n",
        "    ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n",
        "    bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if half else (32, None)\n",
        "    if bits < 32:\n",
        "        if MACOS:  # quantization only supported on macOS\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
        "                ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n",
        "        else:\n",
        "            print(f'{prefix} quantization only supported on macOS, skipping...')\n",
        "    ct_model.save(f)\n",
        "    return f, ct_model\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n",
        "    # YOLOv5 TensorRT export https://developer.nvidia.com/tensorrt\n",
        "    assert im.device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'\n",
        "    try:\n",
        "        import tensorrt as trt\n",
        "    except Exception:\n",
        "        if platform.system() == 'Linux':\n",
        "            check_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\n",
        "        import tensorrt as trt\n",
        "\n",
        "    if trt.__version__[0] == '7':  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012\n",
        "        grid = model.model[-1].anchor_grid\n",
        "        model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]\n",
        "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
        "        model.model[-1].anchor_grid = grid\n",
        "    else:  # TensorRT >= 8\n",
        "        check_version(trt.__version__, '8.0.0', hard=True)  # require tensorrt>=8.0.0\n",
        "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
        "    onnx = file.with_suffix('.onnx')\n",
        "\n",
        "    assert onnx.exists(), f'failed to export ONNX file: {onnx}'\n",
        "    f = file.with_suffix('.engine')  # TensorRT engine file\n",
        "    logger = trt.Logger(trt.Logger.INFO)\n",
        "    if verbose:\n",
        "        logger.min_severity = trt.Logger.Severity.VERBOSE\n",
        "\n",
        "    builder = trt.Builder(logger)\n",
        "    config = builder.create_builder_config()\n",
        "    config.max_workspace_size = workspace * 1 << 30\n",
        "    # config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace << 30)  # fix TRT 8.4 deprecation notice\n",
        "\n",
        "    flag = (1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
        "    network = builder.create_network(flag)\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "    if not parser.parse_from_file(str(onnx)):\n",
        "        raise RuntimeError(f'failed to load ONNX file: {onnx}')\n",
        "\n",
        "    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
        "\n",
        "    if dynamic:\n",
        "        if im.shape[0] <= 1:\n",
        "            pass\n",
        "        profile = builder.create_optimization_profile()\n",
        "        for inp in inputs:\n",
        "            profile.set_shape(inp.name, (1, *im.shape[1:]), (max(1, im.shape[0] // 2), *im.shape[1:]), im.shape)\n",
        "        config.add_optimization_profile(profile)\n",
        "\n",
        "    if builder.platform_has_fast_fp16 and half:\n",
        "        config.set_flag(trt.BuilderFlag.FP16)\n",
        "    with builder.build_engine(network, config) as engine, open(f, 'wb') as t:\n",
        "        t.write(engine.serialize())\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_saved_model(model,\n",
        "                       im,\n",
        "                       file,\n",
        "                       dynamic,\n",
        "                       tf_nms=False,\n",
        "                       agnostic_nms=False,\n",
        "                       topk_per_class=100,\n",
        "                       topk_all=100,\n",
        "                       iou_thres=0.45,\n",
        "                       conf_thres=0.25,\n",
        "                       keras=False,\n",
        "                       prefix=colorstr('TensorFlow SavedModel:')):\n",
        "    # YOLOv5 TensorFlow SavedModel export\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "    except Exception:\n",
        "        check_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}\")\n",
        "        import tensorflow as tf\n",
        "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "    from models.tf import TFModel\n",
        "\n",
        "    f = str(file).replace('.pt', '_saved_model')\n",
        "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
        "\n",
        "    tf_model = TFModel(cfg=model.yaml, model=model, nc=model.nc, imgsz=imgsz)\n",
        "    im = tf.zeros((batch_size, *imgsz, ch))  # BHWC order for TensorFlow\n",
        "    _ = tf_model.predict(im, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
        "    inputs = tf.keras.Input(shape=(*imgsz, ch), batch_size=None if dynamic else batch_size)\n",
        "    outputs = tf_model.predict(inputs, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
        "    keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    keras_model.trainable = False\n",
        "    keras_model.summary()\n",
        "    if keras:\n",
        "        keras_model.save(f, save_format='tf')\n",
        "    else:\n",
        "        spec = tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype)\n",
        "        m = tf.function(lambda x: keras_model(x))  # full model\n",
        "        m = m.get_concrete_function(spec)\n",
        "        frozen_func = convert_variables_to_constants_v2(m)\n",
        "        tfm = tf.Module()\n",
        "        tfm.__call__ = tf.function(lambda x: frozen_func(x)[:4] if tf_nms else frozen_func(x), [spec])\n",
        "        tfm.__call__(im)\n",
        "        tf.saved_model.save(tfm,\n",
        "                            f,\n",
        "                            options=tf.saved_model.SaveOptions(experimental_custom_gradients=False) if check_version(\n",
        "                                tf.__version__, '2.6') else tf.saved_model.SaveOptions())\n",
        "    return f, keras_model\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_pb(keras_model, file, prefix=colorstr('TensorFlow GraphDef:')):\n",
        "    # YOLOv5 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "    f = file.with_suffix('.pb')\n",
        "\n",
        "    m = tf.function(lambda x: keras_model(x))  # full model\n",
        "    m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n",
        "    frozen_func = convert_variables_to_constants_v2(m)\n",
        "    frozen_func.graph.as_graph_def()\n",
        "    tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_tflite(keras_model, im, file, int8, data, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:')):\n",
        "    # YOLOv5 TensorFlow Lite export\n",
        "    import tensorflow as tf\n",
        "\n",
        "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
        "    f = str(file).replace('.pt', '-fp16.tflite')\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    if int8:\n",
        "        from models.tf import representative_dataset_gen\n",
        "        dataset = LoadImages(check_dataset(check_yaml(data))['train'], img_size=imgsz, auto=False)\n",
        "        converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib=100)\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.target_spec.supported_types = []\n",
        "        converter.inference_input_type = tf.uint8  # or tf.int8\n",
        "        converter.inference_output_type = tf.uint8  # or tf.int8\n",
        "        converter.experimental_new_quantizer = True\n",
        "        f = str(file).replace('.pt', '-int8.tflite')\n",
        "    if nms or agnostic_nms:\n",
        "        converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "    open(f, \"wb\").write(tflite_model)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_edgetpu(file, prefix=colorstr('Edge TPU:')):\n",
        "    # YOLOv5 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/\n",
        "    cmd = 'edgetpu_compiler --version'\n",
        "    help_url = 'https://coral.ai/docs/edgetpu/compiler/'\n",
        "    assert platform.system() == 'Linux', f'export only supported on Linux. See {help_url}'\n",
        "    if subprocess.run(f'{cmd} >/dev/null', shell=True).returncode != 0:\n",
        "        sudo = subprocess.run('sudo --version >/dev/null', shell=True).returncode == 0  # sudo installed on system\n",
        "        for c in (\n",
        "                'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',\n",
        "                'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n",
        "                'sudo apt-get update', 'sudo apt-get install edgetpu-compiler'):\n",
        "            subprocess.run(c if sudo else c.replace('sudo ', ''), shell=True, check=True)\n",
        "    ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\n",
        "\n",
        "    f = str(file).replace('.pt', '-int8_edgetpu.tflite')  # Edge TPU model\n",
        "    f_tfl = str(file).replace('.pt', '-int8.tflite')  # TFLite model\n",
        "\n",
        "    cmd = f\"edgetpu_compiler -s -d -k 10 --out_dir {file.parent} {f_tfl}\"\n",
        "    subprocess.run(cmd.split(), check=True)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "@try_export\n",
        "def export_tfjs(file, prefix=colorstr('TensorFlow.js:')):\n",
        "    # YOLOv5 TensorFlow.js export\n",
        "    check_requirements('tensorflowjs')\n",
        "    import tensorflowjs as tfjs\n",
        "\n",
        "    f = str(file).replace('.pt', '_web_model')  # js dir\n",
        "    f_pb = file.with_suffix('.pb')  # *.pb path\n",
        "    f_json = f'{f}/model.json'  # *.json path\n",
        "\n",
        "    cmd = f'tensorflowjs_converter --input_format=tf_frozen_model ' \\\n",
        "          f'--output_node_names=Identity,Identity_1,Identity_2,Identity_3 {f_pb} {f}'\n",
        "    subprocess.run(cmd.split())\n",
        "\n",
        "    json = Path(f_json).read_text()\n",
        "    with open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order\n",
        "        subst = re.sub(\n",
        "            r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
        "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}', r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n",
        "            r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n",
        "            r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n",
        "            r'\"Identity_3\": {\"name\": \"Identity_3\"}}}', json)\n",
        "        j.write(subst)\n",
        "    return f, None\n",
        "\n",
        "\n",
        "def add_tflite_metadata(file, metadata, num_outputs):\n",
        "    # Add metadata to *.tflite models per https://www.tensorflow.org/lite/models/convert/metadata\n",
        "    with contextlib.suppress(ImportError):\n",
        "        # check_requirements('tflite_support')\n",
        "        from tflite_support import flatbuffers\n",
        "        from tflite_support import metadata as _metadata\n",
        "        from tflite_support import metadata_schema_py_generated as _metadata_fb\n",
        "\n",
        "        tmp_file = Path('/tmp/meta.txt')\n",
        "        with open(tmp_file, 'w') as meta_f:\n",
        "            meta_f.write(str(metadata))\n",
        "\n",
        "        model_meta = _metadata_fb.ModelMetadataT()\n",
        "        label_file = _metadata_fb.AssociatedFileT()\n",
        "        label_file.name = tmp_file.name\n",
        "        model_meta.associatedFiles = [label_file]\n",
        "\n",
        "        subgraph = _metadata_fb.SubGraphMetadataT()\n",
        "        subgraph.inputTensorMetadata = [_metadata_fb.TensorMetadataT()]\n",
        "        subgraph.outputTensorMetadata = [_metadata_fb.TensorMetadataT()] * num_outputs\n",
        "        model_meta.subgraphMetadata = [subgraph]\n",
        "\n",
        "        b = flatbuffers.Builder(0)\n",
        "        b.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
        "        metadata_buf = b.Output()\n",
        "\n",
        "        populator = _metadata.MetadataPopulator.with_model_file(file)\n",
        "        populator.load_metadata_buffer(metadata_buf)\n",
        "        populator.load_associated_files([str(tmp_file)])\n",
        "        populator.populate()\n",
        "        tmp_file.unlink()\n",
        "\n",
        "\n",
        "@smart_inference_mode()\n",
        "def run(\n",
        "        data=ROOT / 'data/coco128.yaml',  # 'dataset.yaml path'\n",
        "        weights=ROOT / 'yolov5s.pt',  # weights path\n",
        "        imgsz=(640, 640),  # image (height, width)\n",
        "        batch_size=1,  # batch size\n",
        "        device='cpu',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        include=('torchscript', 'onnx'),  # include formats\n",
        "        half=False,  # FP16 half-precision export\n",
        "        inplace=False,  # set YOLOv5 Detect() inplace=True\n",
        "        keras=False,  # use Keras\n",
        "        optimize=False,  # TorchScript: optimize for mobile\n",
        "        int8=False,  # CoreML/TF INT8 quantization\n",
        "        dynamic=False,  # ONNX/TF/TensorRT: dynamic axes\n",
        "        simplify=False,  # ONNX: simplify model\n",
        "        opset=12,  # ONNX: opset version\n",
        "        verbose=False,  # TensorRT: verbose log\n",
        "        workspace=4,  # TensorRT: workspace size (GB)\n",
        "        nms=False,  # TF: add NMS to model\n",
        "        agnostic_nms=False,  # TF: add agnostic NMS to model\n",
        "        topk_per_class=100,  # TF.js NMS: topk per class to keep\n",
        "        topk_all=100,  # TF.js NMS: topk for all classes to keep\n",
        "        iou_thres=0.45,  # TF.js NMS: IoU threshold\n",
        "        conf_thres=0.25,  # TF.js NMS: confidence threshold\n",
        "):\n",
        "    t = time.time()\n",
        "    include = [x.lower() for x in include]  # to lowercase\n",
        "    fmts = tuple(export_formats()['Argument'][1:])  # --include arguments\n",
        "    flags = [x in include for x in fmts]\n",
        "    assert sum(flags) == len(include), f'ERROR: Invalid --include {include}, valid --include arguments are {fmts}'\n",
        "    jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle = flags  # export booleans\n",
        "    file = Path(url2file(weights) if str(weights).startswith(('http:/', 'https:/')) else weights)  # PyTorch weights\n",
        "\n",
        "    # Load PyTorch model\n",
        "    device = select_device(device)\n",
        "    if half:\n",
        "        assert device.type != 'cpu' or coreml, '--half only compatible with GPU export, i.e. use --device 0'\n",
        "        assert not dynamic, '--half not compatible with --dynamic, i.e. use either --half or --dynamic but not both'\n",
        "    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n",
        "\n",
        "    # Checks\n",
        "    imgsz *= 2 if len(imgsz) == 1 else 1  # expand\n",
        "    if optimize:\n",
        "        assert device.type == 'cpu', '--optimize not compatible with cuda devices, i.e. use --device cpu'\n",
        "\n",
        "    # Input\n",
        "    gs = int(max(model.stride))  # grid size (max stride)\n",
        "    imgsz = [check_img_size(x, gs) for x in imgsz]  # verify img_size are gs-multiples\n",
        "    im = torch.zeros(batch_size, 3, *imgsz).to(device)  # image size(1,3,320,192) BCHW iDetection\n",
        "\n",
        "    # Update model\n",
        "    model.eval()\n",
        "    for k, m in model.named_modules():\n",
        "        if isinstance(m, Detect):\n",
        "            m.inplace = inplace\n",
        "            m.dynamic = dynamic\n",
        "            m.export = True\n",
        "\n",
        "    for _ in range(2):\n",
        "        y = model(im)  # dry runs\n",
        "    if half and not coreml:\n",
        "        im, model = im.half(), model.half()  # to FP16\n",
        "    shape = tuple((y[0] if isinstance(y, tuple) else y).shape)  # model output shape\n",
        "    metadata = {'stride': int(max(model.stride)), 'names': model.names}  # model metadata\n",
        "\n",
        "    # Exports\n",
        "    f = [''] * len(fmts)  # exported filenames\n",
        "    warnings.filterwarnings(action='ignore', category=torch.jit.TracerWarning)  # suppress TracerWarning\n",
        "    if jit:  # TorchScript\n",
        "        f[0], _ = export_torchscript(model, im, file, optimize)\n",
        "    if engine:  # TensorRT required before ONNX\n",
        "        f[1], _ = export_engine(model, im, file, half, dynamic, simplify, workspace, verbose)\n",
        "    if onnx or xml:  # OpenVINO requires ONNX\n",
        "        f[2], _ = export_onnx(model, im, file, opset, dynamic, simplify)\n",
        "    if xml:  # OpenVINO\n",
        "        f[3], _ = export_openvino(file, metadata, half)\n",
        "    if coreml:  # CoreML\n",
        "        f[4], _ = export_coreml(model, im, file, int8, half)\n",
        "    if any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\n",
        "        assert not tflite or not tfjs, 'TFLite and TF.js models must be exported separately, please pass only one type.'\n",
        "        assert not isinstance(model, ClassificationModel), 'ClassificationModel export to TF formats not yet supported.'\n",
        "        f[5], s_model = export_saved_model(model.cpu(),\n",
        "                                           im,\n",
        "                                           file,\n",
        "                                           dynamic,\n",
        "                                           tf_nms=nms or agnostic_nms or tfjs,\n",
        "                                           agnostic_nms=agnostic_nms or tfjs,\n",
        "                                           topk_per_class=topk_per_class,\n",
        "                                           topk_all=topk_all,\n",
        "                                           iou_thres=iou_thres,\n",
        "                                           conf_thres=conf_thres,\n",
        "                                           keras=keras)\n",
        "        if pb or tfjs:  # pb prerequisite to tfjs\n",
        "            f[6], _ = export_pb(s_model, file)\n",
        "        if tflite or edgetpu:\n",
        "            f[7], _ = export_tflite(s_model, im, file, int8 or edgetpu, data=data, nms=nms, agnostic_nms=agnostic_nms)\n",
        "            if edgetpu:\n",
        "                f[8], _ = export_edgetpu(file)\n",
        "            add_tflite_metadata(f[8] or f[7], metadata, num_outputs=len(s_model.outputs))\n",
        "        if tfjs:\n",
        "            f[9], _ = export_tfjs(file)\n",
        "    if paddle:  # PaddlePaddle\n",
        "        f[10], _ = export_paddle(model, im, file, metadata)\n",
        "\n",
        "    # Finish\n",
        "    f = [str(x) for x in f if x]  # filter out '' and None\n",
        "    if any(f):\n",
        "        cls, det, seg = (isinstance(model, x) for x in (ClassificationModel, DetectionModel, SegmentationModel))  # type\n",
        "        dir = Path('segment' if seg else 'classify' if cls else '')\n",
        "        h = '--half' if half else ''  # --half FP16 inference arg\n",
        "        s = \"# WARNING ⚠️ ClassificationModel not yet supported for PyTorch Hub AutoShape inference\" if cls else \\\n",
        "            \"# WARNING ⚠️ SegmentationModel not yet supported for PyTorch Hub AutoShape inference\" if seg else ''\n",
        "\n",
        "    return f  # return list of exported files/dirs\n",
        "\n",
        "\n",
        "def parse_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640, 640], help='image (h, w)')\n",
        "    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n",
        "    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n",
        "    parser.add_argument('--inplace', action='store_true', help='set YOLOv5 Detect() inplace=True')\n",
        "    parser.add_argument('--keras', action='store_true', help='TF: use Keras')\n",
        "    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile')\n",
        "    parser.add_argument('--int8', action='store_true', help='CoreML/TF INT8 quantization')\n",
        "    parser.add_argument('--dynamic', action='store_true', help='ONNX/TF/TensorRT: dynamic axes')\n",
        "    parser.add_argument('--simplify', action='store_true', help='ONNX: simplify model')\n",
        "    parser.add_argument('--opset', type=int, default=12, help='ONNX: opset version')\n",
        "    parser.add_argument('--verbose', action='store_true', help='TensorRT: verbose log')\n",
        "    parser.add_argument('--workspace', type=int, default=4, help='TensorRT: workspace size (GB)')\n",
        "    parser.add_argument('--nms', action='store_true', help='TF: add NMS to model')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='TF: add agnostic NMS to model')\n",
        "    parser.add_argument('--topk-per-class', type=int, default=100, help='TF.js NMS: topk per class to keep')\n",
        "    parser.add_argument('--topk-all', type=int, default=100, help='TF.js NMS: topk for all classes to keep')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='TF.js NMS: IoU threshold')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='TF.js NMS: confidence threshold')\n",
        "    parser.add_argument(\n",
        "        '--include',\n",
        "        nargs='+',\n",
        "        default=['torchscript'],\n",
        "        help='torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle')\n",
        "    opt = parser.parse_args()\n",
        "    print_args(vars(opt))\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    for opt.weights in (opt.weights if isinstance(opt.weights, list) else [opt.weights]):\n",
        "        run(**vars(opt))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    main(opt)\n",
        "\"\"\"\n",
        "\n",
        "path_w = 'export.py'\n",
        "\n",
        "with open(path_w, mode='w') as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "fV6p2Qg3GJmi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True) \n",
        "\n",
        "# Change --weiths path to the path to your own checkpoint \n",
        "!python export.py --weights /content/yolov5/yolov5s.pt --include \"coreml\"\n",
        "# After run this, mlmodel saved in the directory same with the weight/"
      ],
      "metadata": {
        "id": "ZLh2PI5fAPi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just run to define the decode function\n",
        "import torch\n",
        "# classLabels = [f\"label{i}\" for i in range(80)]\n",
        "numberOfClassLabels = len(classLabels)\n",
        "outputSize = numberOfClassLabels + 5\n",
        "\n",
        "#  Attention: Some models are reversed!\n",
        "reverseModel = False\n",
        "\n",
        "strides = [8, 16, 32]\n",
        "if reverseModel:\n",
        "    strides.reverse()\n",
        "featureMapDimensions = [640 // stride for stride in strides]\n",
        "\n",
        "anchors = ([10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [\n",
        "           116, 90, 156, 198, 373, 326])  # Take these from the <model>.yml in yolov5\n",
        "if reverseModel:\n",
        "    anchors = anchors[::-1]\n",
        "\n",
        "anchorGrid = torch.tensor(anchors).float().view(3, -1, 1, 1, 2)\n",
        "\n",
        "def make_grid(nx, ny):\n",
        "    yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
        "    return torch.stack((xv, yv), 2).view((ny, nx, 2)).float()\n",
        "\n",
        "def addExportLayerToCoreml(builder):\n",
        "    '''\n",
        "    Adds the yolov5 export layer to the coreml model\n",
        "    '''\n",
        "    outputNames = [output.name for output in builder.spec.description.output]\n",
        "\n",
        "    for i, outputName in enumerate(outputNames):\n",
        "        # formulas: https://github.com/ultralytics/yolov5/issues/471\n",
        "        builder.add_activation(name=f\"sigmoid_{outputName}\", non_linearity=\"SIGMOID\",\n",
        "                               input_name=outputName, output_name=f\"{outputName}_sigmoid\")\n",
        "\n",
        "        ### Coordinates calculation ###\n",
        "        # input (1, 3, nC, nC, 85), output (1, 3, nC, nC, 2) -> nC = 640 / strides[i]\n",
        "        builder.add_slice(name=f\"slice_coordinates_xy_{outputName}\", input_name=f\"{outputName}_sigmoid\",\n",
        "                          output_name=f\"{outputName}_sliced_coordinates_xy\", axis=\"width\", start_index=0, end_index=2)\n",
        "        # x,y * 2\n",
        "        builder.add_elementwise(name=f\"multiply_xy_by_two_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_sliced_coordinates_xy\"], output_name=f\"{outputName}_multiplied_xy_by_two\", mode=\"MULTIPLY\", alpha=2)\n",
        "        # x,y * 2 - 0.5\n",
        "        builder.add_elementwise(name=f\"subtract_0_5_from_xy_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_multiplied_xy_by_two\"], output_name=f\"{outputName}_subtracted_0_5_from_xy\", mode=\"ADD\", alpha=-0.5)\n",
        "        grid = make_grid(\n",
        "            featureMapDimensions[i], featureMapDimensions[i]).numpy()\n",
        "        # x,y * 2 - 0.5 + grid[i]\n",
        "        builder.add_bias(name=f\"add_grid_from_xy_{outputName}\", input_name=f\"{outputName}_subtracted_0_5_from_xy\",\n",
        "                         output_name=f\"{outputName}_added_grid_xy\", b=grid, shape_bias=grid.shape)\n",
        "        # (x,y * 2 - 0.5 + grid[i]) * stride[i]\n",
        "        builder.add_elementwise(name=f\"multiply_xy_by_stride_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_added_grid_xy\"], output_name=f\"{outputName}_calculated_xy\", mode=\"MULTIPLY\", alpha=strides[i])\n",
        "\n",
        "        # input (1, 3, nC, nC, 85), output (1, 3, nC, nC, 2)\n",
        "        builder.add_slice(name=f\"slice_coordinates_wh_{outputName}\", input_name=f\"{outputName}_sigmoid\",\n",
        "                          output_name=f\"{outputName}_sliced_coordinates_wh\", axis=\"width\", start_index=2, end_index=4)\n",
        "        # w,h * 2\n",
        "        builder.add_elementwise(name=f\"multiply_wh_by_two_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_sliced_coordinates_wh\"], output_name=f\"{outputName}_multiplied_wh_by_two\", mode=\"MULTIPLY\", alpha=2)\n",
        "        # (w,h * 2) ** 2\n",
        "        builder.add_unary(name=f\"power_wh_{outputName}\", input_name=f\"{outputName}_multiplied_wh_by_two\",\n",
        "                          output_name=f\"{outputName}_power_wh\", mode=\"power\", alpha=2)\n",
        "        # (w,h * 2) ** 2 * anchor_grid[i]\n",
        "        anchor = anchorGrid[i].expand(-1, featureMapDimensions[i],\n",
        "                                      featureMapDimensions[i], -1).numpy()\n",
        "        builder.add_load_constant_nd(\n",
        "            name=f\"anchors_{outputName}\", output_name=f\"{outputName}_anchors\", constant_value=anchor, shape=anchor.shape)\n",
        "        builder.add_elementwise(name=f\"multiply_wh_with_achors_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_power_wh\", f\"{outputName}_anchors\"], output_name=f\"{outputName}_calculated_wh\", mode=\"MULTIPLY\")\n",
        "\n",
        "        builder.add_concat_nd(name=f\"concat_coordinates_{outputName}\", input_names=[\n",
        "                              f\"{outputName}_calculated_xy\", f\"{outputName}_calculated_wh\"], output_name=f\"{outputName}_raw_coordinates\", axis=-1)\n",
        "        builder.add_scale(name=f\"normalize_coordinates_{outputName}\", input_name=f\"{outputName}_raw_coordinates\",\n",
        "                          output_name=f\"{outputName}_raw_normalized_coordinates\", W=torch.tensor([1 / 640]).numpy(), b=0, has_bias=False)\n",
        "\n",
        "        ### Confidence calculation ###\n",
        "        builder.add_slice(name=f\"slice_object_confidence_{outputName}\", input_name=f\"{outputName}_sigmoid\",\n",
        "                          output_name=f\"{outputName}_object_confidence\", axis=\"width\", start_index=4, end_index=5)\n",
        "        builder.add_slice(name=f\"slice_label_confidence_{outputName}\", input_name=f\"{outputName}_sigmoid\",\n",
        "                          output_name=f\"{outputName}_label_confidence\", axis=\"width\", start_index=5, end_index=0)\n",
        "        # confidence = object_confidence * label_confidence\n",
        "        builder.add_multiply_broadcastable(name=f\"multiply_object_label_confidence_{outputName}\", input_names=[\n",
        "                                           f\"{outputName}_label_confidence\", f\"{outputName}_object_confidence\"], output_name=f\"{outputName}_raw_confidence\")\n",
        "\n",
        "        # input: (1, 3, nC, nC, 85), output: (3 * nc^2, 85)\n",
        "        builder.add_flatten_to_2d(\n",
        "            name=f\"flatten_confidence_{outputName}\", input_name=f\"{outputName}_raw_confidence\", output_name=f\"{outputName}_flatten_raw_confidence\", axis=-1)\n",
        "        builder.add_flatten_to_2d(\n",
        "            name=f\"flatten_coordinates_{outputName}\", input_name=f\"{outputName}_raw_normalized_coordinates\", output_name=f\"{outputName}_flatten_raw_coordinates\", axis=-1)\n",
        "\n",
        "    builder.add_concat_nd(name=\"concat_confidence\", input_names=[\n",
        "                          f\"{outputName}_flatten_raw_confidence\" for outputName in outputNames], output_name=\"raw_confidence\", axis=-2)\n",
        "    builder.add_concat_nd(name=\"concat_coordinates\", input_names=[\n",
        "                          f\"{outputName}_flatten_raw_coordinates\" for outputName in outputNames], output_name=\"raw_coordinates\", axis=-2)\n",
        "\n",
        "    builder.set_output(output_names=[\"raw_confidence\", \"raw_coordinates\"], output_dims=[\n",
        "                       (25200, numberOfClassLabels), (25200, 4)])"
      ],
      "metadata": {
        "id": "Hpigyc_5Fbri"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just run to define the NMS function\n",
        "\n",
        "def createNmsModelSpec(nnSpec):\n",
        "    '''\n",
        "    Create a coreml model with nms to filter the results of the model\n",
        "    '''\n",
        "    nmsSpec = ct.proto.Model_pb2.Model()\n",
        "    nmsSpec.specificationVersion = 4\n",
        "\n",
        "    # Define input and outputs of the model\n",
        "    for i in range(2):\n",
        "        nnOutput = nnSpec.description.output[i].SerializeToString()\n",
        "\n",
        "        nmsSpec.description.input.add()\n",
        "        nmsSpec.description.input[i].ParseFromString(nnOutput)\n",
        "\n",
        "        nmsSpec.description.output.add()\n",
        "        nmsSpec.description.output[i].ParseFromString(nnOutput)\n",
        "\n",
        "    nmsSpec.description.output[0].name = \"confidence\"\n",
        "    nmsSpec.description.output[1].name = \"coordinates\"\n",
        "\n",
        "    # Define output shape of the model\n",
        "    outputSizes = [numberOfClassLabels, 4]\n",
        "    for i in range(len(outputSizes)):\n",
        "        maType = nmsSpec.description.output[i].type.multiArrayType\n",
        "        # First dimension of both output is the number of boxes, which should be flexible\n",
        "        maType.shapeRange.sizeRanges.add()\n",
        "        maType.shapeRange.sizeRanges[0].lowerBound = 0\n",
        "        maType.shapeRange.sizeRanges[0].upperBound = -1\n",
        "        # Second dimension is fixed, for \"confidence\" it's the number of classes, for coordinates it's position (x, y) and size (w, h)\n",
        "        maType.shapeRange.sizeRanges.add()\n",
        "        maType.shapeRange.sizeRanges[1].lowerBound = outputSizes[i]\n",
        "        maType.shapeRange.sizeRanges[1].upperBound = outputSizes[i]\n",
        "        del maType.shape[:]\n",
        "\n",
        "    # Define the model type non maximum supression\n",
        "    nms = nmsSpec.nonMaximumSuppression\n",
        "    nms.confidenceInputFeatureName = \"raw_confidence\"\n",
        "    nms.coordinatesInputFeatureName = \"raw_coordinates\"\n",
        "    nms.confidenceOutputFeatureName = \"confidence\"\n",
        "    nms.coordinatesOutputFeatureName = \"coordinates\"\n",
        "    nms.iouThresholdInputFeatureName = \"iouThreshold\"\n",
        "    nms.confidenceThresholdInputFeatureName = \"confidenceThreshold\"\n",
        "    # Some good default values for the two additional inputs, can be overwritten when using the model\n",
        "    nms.iouThreshold = 0.4\n",
        "    nms.confidenceThreshold = 0.25\n",
        "    nms.stringClassLabels.vector.extend(classLabels)\n",
        "\n",
        "    return nmsSpec\n"
      ],
      "metadata": {
        "id": "g9rgzl0_DrxT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just run to combine the model added decode and the NMS.\n",
        "def combineModelsAndExport(builderSpec, nmsSpec, fileName, quantize=False):\n",
        "    '''\n",
        "    Combines the coreml model with export logic and the nms to one final model. Optionally save with different quantization (32, 16, 8) (Works only if on Mac Os)\n",
        "    '''\n",
        "    try:\n",
        "        print(f'Combine CoreMl model with nms and export model')\n",
        "        # Combine models to a single one\n",
        "        pipeline = ct.models.pipeline.Pipeline(input_features=[(\"image\", ct.models.datatypes.Array(3, 460, 460)),\n",
        "                                                               (\"iouThreshold\", ct.models.datatypes.Double(\n",
        "                                                               )),\n",
        "                                                               (\"confidenceThreshold\", ct.models.datatypes.Double())], output_features=[\"confidence\", \"coordinates\"])\n",
        "\n",
        "        # Required version (>= ios13) in order for mns to work\n",
        "        pipeline.spec.specificationVersion = 4\n",
        "\n",
        "        pipeline.add_model(builderSpec)\n",
        "        pipeline.add_model(nmsSpec)\n",
        "\n",
        "        pipeline.spec.description.input[0].ParseFromString(\n",
        "            builderSpec.description.input[0].SerializeToString())\n",
        "        pipeline.spec.description.output[0].ParseFromString(\n",
        "            nmsSpec.description.output[0].SerializeToString())\n",
        "        pipeline.spec.description.output[1].ParseFromString(\n",
        "            nmsSpec.description.output[1].SerializeToString())\n",
        "\n",
        "        # Metadata for the model‚\n",
        "        pipeline.spec.description.input[\n",
        "            1].shortDescription = \"(optional) IOU Threshold override (Default: 0.6)\"\n",
        "        pipeline.spec.description.input[\n",
        "            2].shortDescription = \"(optional) Confidence Threshold override (Default: 0.4)\"\n",
        "        pipeline.spec.description.output[0].shortDescription = u\"Boxes \\xd7 Class confidence\"\n",
        "        pipeline.spec.description.output[\n",
        "            1].shortDescription = u\"Boxes \\xd7 [x, y, width, height] (relative to image size)\"\n",
        "        pipeline.spec.description.metadata.versionString = \"1.0\"\n",
        "        pipeline.spec.description.metadata.shortDescription = \"yolov5\"\n",
        "        pipeline.spec.description.metadata.author = \"Leon De Andrade\"\n",
        "        pipeline.spec.description.metadata.license = \"\"\n",
        "\n",
        "        model = ct.models.MLModel(pipeline.spec)\n",
        "        model.save(fileName)\n",
        "\n",
        "        if quantize:\n",
        "            fileName16 = fileName.replace(\".mlmodel\", \"_16.mlmodel\")\n",
        "            modelFp16 = ct.models.neural_network.quantization_utils.quantize_weights(\n",
        "                model, nbits=16)\n",
        "            modelFp16.save(fileName16)\n",
        "\n",
        "            fileName8 = fileName.replace(\".mlmodel\", \"_8.mlmodel\")\n",
        "            modelFp8 = ct.models.neural_network.quantization_utils.quantize_weights(\n",
        "                model, nbits=8)\n",
        "            modelFp8.save(fileName8)\n",
        "\n",
        "        print(f'CoreML export success, saved as {fileName}')\n",
        "    except Exception as e:\n",
        "        print(f'CoreML export failure: {e}')"
      ],
      "metadata": {
        "id": "gq8rtvsbEDYm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You need specify the path to your model that converted and saved in the same folder of your weight file.\n",
        "import coremltools as ct\n",
        "mlmodel = ct.models.MLModel(\"/content/yolov5/yolov5s.mlmodel\")"
      ],
      "metadata": {
        "id": "eUthrL2h-CEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14ef331-c917-4b3f-9b36-fd8909d2b811"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TensorFlow version 2.9.2 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.8.0 is the most recent version that has been tested.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just run to get the mlmodel spec.\n",
        "spec = mlmodel.get_spec()\n",
        "builder = ct.models.neural_network.NeuralNetworkBuilder(spec=spec)\n",
        "spec.description"
      ],
      "metadata": {
        "id": "DRZi7EcuxCTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the functions to add decode layer and NMS to the model.\n",
        "addExportLayerToCoreml(builder)\n",
        "nmsSpec = createNmsModelSpec(builder.spec)\n",
        "combineModelsAndExport(builder.spec, nmsSpec, f\"/content/yolov5/yolov5s.mlmodel\") # The model will be saved in this path."
      ],
      "metadata": {
        "id": "3x5Mbli7Dk0v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "86f9df20-080c-4701-8efc-33b8cdd556cc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-78f4f4fd1bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run the functions to add decode layer and NMS to the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maddExportLayerToCoreml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnmsSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateNmsModelSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcombineModelsAndExport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmsSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"/content/yolov5/yolov5s.mlmodel\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The model will be saved in this path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-b01750cbb15f>\u001b[0m in \u001b[0;36maddExportLayerToCoreml\u001b[0;34m(builder)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     builder.set_output(output_names=[\"raw_confidence\", \"raw_coordinates\"], output_dims=[\n\u001b[0;32m---> 99\u001b[0;31m                        (25200, numberOfClassLabels), (25200, 4)])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/coremltools/models/neural_network/builder.py\u001b[0m in \u001b[0;36mset_output\u001b[0;34m(self, output_names, output_dims)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiArrayType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClearField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiArrayType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             spec.description.output[\n",
            "\u001b[0;31mIndexError\u001b[0m: list index (1) out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sgkuhH2ind6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!git clone https://github.com/dbsystel/yolov5-coreml-tools.git\n",
        "os.chdir(\"yolov5-coreml-tools\")\n",
        "!mkdir models\n",
        "os.chdir(\"models\")\n",
        "!curl -OL https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5s.pt\n",
        "!mv yolov5s.pt yolov5s_v4.pt\n",
        "os.chdir(\"/content\")\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "os.chdir(\"/content/yolov5\")\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZslCHSdGXdk",
        "outputId": "b0c028e0-3824-4442-904d-135ff7d25afa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5-coreml-tools'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 81 (delta 33), reused 78 (delta 31), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (81/81), done.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 14.1M  100 14.1M    0     0  13.3M      0  0:00:01  0:00:01 --:--:-- 13.3M\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 14596, done.\u001b[K\n",
            "remote: Counting objects: 100% (150/150), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 14596 (delta 95), reused 99 (delta 52), pack-reused 14446\u001b[K\n",
            "Receiving objects: 100% (14596/14596), 13.50 MiB | 20.88 MiB/s, done.\n",
            "Resolving deltas: 100% (10068/10068), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (4.64.1)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (2.9.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (0.11.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 41)) (7.9.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 42)) (5.4.8)\n",
            "Collecting thop>=0.1.1\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 12)) (4.1.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.50.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.0.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 23)) (2022.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.2.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 41)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 41)) (2.0.10)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 41)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 41)) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 41)) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 41)) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 41)) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->-r requirements.txt (line 41)) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->-r requirements.txt (line 41)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->-r requirements.txt (line 41)) (0.7.0)\n",
            "Installing collected packages: jedi, thop\n",
            "Successfully installed jedi-0.18.1 thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/yolov5\")\n",
        "text = \"\"\"\\\n",
        "from setuptools import setup\n",
        "\n",
        "setup(\n",
        "    name=\"yolov5\",\n",
        "    url=\"https://github.com/ultralytics/yolov5\",\n",
        "    maintainer=\"ultralytics\",\n",
        "    maintainer_email=\"glenn.jocher@ultralytics.com\",\n",
        "    packages=[\"models\", \"utils\"],\n",
        "    install_requires=[\"opencv-python\", \"matplotlib\", \"torchvision\", \"PyYAML\", \"requests\", \"pandas\", \"seaborn\"],\n",
        ")\n",
        "\"\"\"\n",
        "path_w = 'setup.py'\n",
        "\n",
        "with open(path_w, mode='w') as f:\n",
        "    f.write(text)\n"
      ],
      "metadata": {
        "id": "S1kZoBwDUCUc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/yolov5-coreml-tools/src/coreml_export\")\n",
        "text = \"\"\"\\\n",
        "\n",
        "# Copyright (C) 2021 DB Systel GmbH.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import torch\n",
        "import coremltools as ct\n",
        "from argparse import ArgumentParser\n",
        "from pathlib import Path\n",
        "\n",
        "# Add silu function for yolov5s v4 model: https://github.com/apple/coremltools/issues/1099\n",
        "from coremltools.converters.mil import Builder as mb\n",
        "from coremltools.converters.mil import register_torch_op\n",
        "from coremltools.converters.mil.frontend.torch.ops import _get_inputs\n",
        "\n",
        "@register_torch_op\n",
        "def silu(context, node):\n",
        "    inputs = _get_inputs(context, node, expected=1)\n",
        "    x = inputs[0]\n",
        "    y = mb.sigmoid(x=x)\n",
        "    z = mb.mul(x=x, y=y, name=node.name)\n",
        "    context.add(z)\n",
        "\n",
        "# The labels of your model, pretrained YOLOv5 models usually use the coco dataset and have 80 classes\n",
        "classLabels = [f\"label{i}\" for i in range(80)]\n",
        "numberOfClassLabels = len(classLabels)\n",
        "outputSize = numberOfClassLabels + 5\n",
        "\n",
        "#  Attention: Some models are reversed!\n",
        "reverseModel = False\n",
        "\n",
        "strides = [8, 16, 32]\n",
        "if reverseModel:\n",
        "    strides.reverse()\n",
        "featureMapDimensions = [640 // stride for stride in strides]\n",
        "\n",
        "anchors = ([10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [\n",
        "           116, 90, 156, 198, 373, 326])  # Take these from the <model>.yml in yolov5\n",
        "if reverseModel:\n",
        "    anchors = anchors[::-1]\n",
        "\n",
        "anchorGrid = torch.tensor(anchors).float().view(3, -1, 1, 1, 2)\n",
        "\n",
        "\n",
        "def make_grid(nx, ny):\n",
        "    yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
        "    return torch.stack((xv, yv), 2).view((ny, nx, 2)).float()\n",
        "\n",
        "\n",
        "def exportTorchscript(model, sampleInput, checkInputs, fileName):\n",
        "    '''\n",
        "    Traces a pytorch model and produces a TorchScript\n",
        "    '''\n",
        "    try:\n",
        "        print(f'Starting TorchScript export with torch {torch.__version__}')\n",
        "        ts = torch.jit.trace(model, sampleInput, check_inputs=checkInputs)\n",
        "        ts.save(fileName)\n",
        "        print(f'TorchScript export success, saved as {fileName}')\n",
        "        return ts\n",
        "    except Exception as e:\n",
        "        print(f'TorchScript export failure: {e}')\n",
        "\n",
        "\n",
        "def convertToCoremlSpec(torchScript, sampleInput):\n",
        "    '''\n",
        "    Converts a torchscript to a coreml model\n",
        "    '''\n",
        "    try:\n",
        "        print(f'Starting CoreML conversion with coremltools {ct.__version__}')\n",
        "        nnSpec = ct.convert(torchScript, inputs=[ct.ImageType(\n",
        "            name='image', shape=sampleInput.shape, scale=1 / 255.0, bias=[0, 0, 0])]).get_spec()\n",
        "\n",
        "        print(f'CoreML conversion success')\n",
        "    except Exception as e:\n",
        "        print(f'CoreML conversion failure: {e}')\n",
        "        return\n",
        "    return nnSpec\n",
        "\n",
        "\n",
        "def addOutputMetaData(nnSpec):\n",
        "    '''\n",
        "    Adds the correct output shapes and data types to the coreml model\n",
        "    '''\n",
        "    for i, featureMapDimension in enumerate(featureMapDimensions):\n",
        "        nnSpec.description.output[i].type.multiArrayType.shape.append(1)\n",
        "        nnSpec.description.output[i].type.multiArrayType.shape.append(3)\n",
        "        nnSpec.description.output[i].type.multiArrayType.shape.append(\n",
        "            featureMapDimension)\n",
        "        nnSpec.description.output[i].type.multiArrayType.shape.append(\n",
        "            featureMapDimension)\n",
        "        # pc, bx, by, bh, bw, c (no of class class labels)\n",
        "        nnSpec.description.output[i].type.multiArrayType.shape.append(\n",
        "            outputSize)\n",
        "        nnSpec.description.output[i].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.DOUBLE\n",
        "\n",
        "\n",
        "def addExportLayerToCoreml(builder):\n",
        "    '''\n",
        "    Adds the yolov5 export layer to the coreml model\n",
        "    '''\n",
        "    outputNames = [output.name for output in builder.spec.description.output]\n",
        "\n",
        "    for i, outputName in enumerate(outputNames):\n",
        "        # formulas: https://github.com/ultralytics/yolov5/issues/471\n",
        "        builder.add_activation(name=f\"sigmoid_{outputName}\", non_linearity=\"SIGMOID\",\n",
        "                               input_name=outputName, output_name=f\"{outputName}_sigmoid\")\n",
        "\n",
        "        ### Coordinates calculation ###\n",
        "        # input (1, 3, nC, nC, 85), output (1, 3, nC, nC, 2) -> nC = 640 / strides[i]\n",
        "        builder.add_slice(name=f\"slice_coordinates_xy_{outputName}\", input_name=f\"{outputName}_sigmoid\",\n",
        "                          output_name=f\"{outputName}_sliced_coordinates_xy\", axis=\"width\", start_index=0, end_index=2)\n",
        "        # x,y * 2\n",
        "        builder.add_elementwise(name=f\"multiply_xy_by_two_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_sliced_coordinates_xy\"], output_name=f\"{outputName}_multiplied_xy_by_two\", mode=\"MULTIPLY\", alpha=2)\n",
        "        # x,y * 2 - 0.5\n",
        "        builder.add_elementwise(name=f\"subtract_0_5_from_xy_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_multiplied_xy_by_two\"], output_name=f\"{outputName}_subtracted_0_5_from_xy\", mode=\"ADD\", alpha=-0.5)\n",
        "        grid = make_grid(\n",
        "            featureMapDimensions[i], featureMapDimensions[i]).numpy()\n",
        "        # x,y * 2 - 0.5 + grid[i]\n",
        "        builder.add_bias(name=f\"add_grid_from_xy_{outputName}\", input_name=f\"{outputName}_subtracted_0_5_from_xy\",\n",
        "                         output_name=f\"{outputName}_added_grid_xy\", b=grid, shape_bias=grid.shape)\n",
        "        # (x,y * 2 - 0.5 + grid[i]) * stride[i]\n",
        "        builder.add_elementwise(name=f\"multiply_xy_by_stride_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_added_grid_xy\"], output_name=f\"{outputName}_calculated_xy\", mode=\"MULTIPLY\", alpha=strides[i])\n",
        "\n",
        "        # input (1, 3, nC, nC, 85), output (1, 3, nC, nC, 2)\n",
        "        builder.add_slice(name=f\"slice_coordinates_wh_{outputName}\", input_name=f\"{outputName}_sigmoid\",\n",
        "                          output_name=f\"{outputName}_sliced_coordinates_wh\", axis=\"width\", start_index=2, end_index=4)\n",
        "        # w,h * 2\n",
        "        builder.add_elementwise(name=f\"multiply_wh_by_two_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_sliced_coordinates_wh\"], output_name=f\"{outputName}_multiplied_wh_by_two\", mode=\"MULTIPLY\", alpha=2)\n",
        "        # (w,h * 2) ** 2\n",
        "        builder.add_unary(name=f\"power_wh_{outputName}\", input_name=f\"{outputName}_multiplied_wh_by_two\",\n",
        "                          output_name=f\"{outputName}_power_wh\", mode=\"power\", alpha=2)\n",
        "        # (w,h * 2) ** 2 * anchor_grid[i]\n",
        "        anchor = anchorGrid[i].expand(-1, featureMapDimensions[i],\n",
        "                                      featureMapDimensions[i], -1).numpy()\n",
        "        builder.add_load_constant_nd(\n",
        "            name=f\"anchors_{outputName}\", output_name=f\"{outputName}_anchors\", constant_value=anchor, shape=anchor.shape)\n",
        "        builder.add_elementwise(name=f\"multiply_wh_with_achors_{outputName}\", input_names=[\n",
        "                                f\"{outputName}_power_wh\", f\"{outputName}_anchors\"], output_name=f\"{outputName}_calculated_wh\", mode=\"MULTIPLY\")\n",
        "\n",
        "        builder.add_concat_nd(name=f\"concat_coordinates_{outputName}\", input_names=[\n",
        "                              f\"{outputName}_calculated_xy\", f\"{outputName}_calculated_wh\"], output_name=f\"{outputName}_raw_coordinates\", axis=-1)\n",
        "        builder.add_scale(name=f\"normalize_coordinates_{outputName}\", input_name=f\"{outputName}_raw_coordinates\",\n",
        "                          output_name=f\"{outputName}_raw_normalized_coordinates\", W=torch.tensor([1 / 640]).numpy(), b=0, has_bias=False)\n",
        "\n",
        "        ### Confidence calculation ###\n",
        "        builder.add_slice(name=f\"slice_object_confidence_{outputName}\", input_name=f\"{outputName}_sigmoid\",\n",
        "                          output_name=f\"{outputName}_object_confidence\", axis=\"width\", start_index=4, end_index=5)\n",
        "        builder.add_slice(name=f\"slice_label_confidence_{outputName}\", input_name=f\"{outputName}_sigmoid\",\n",
        "                          output_name=f\"{outputName}_label_confidence\", axis=\"width\", start_index=5, end_index=0)\n",
        "        # confidence = object_confidence * label_confidence\n",
        "        builder.add_multiply_broadcastable(name=f\"multiply_object_label_confidence_{outputName}\", input_names=[\n",
        "                                           f\"{outputName}_label_confidence\", f\"{outputName}_object_confidence\"], output_name=f\"{outputName}_raw_confidence\")\n",
        "\n",
        "        # input: (1, 3, nC, nC, 85), output: (3 * nc^2, 85)\n",
        "        builder.add_flatten_to_2d(\n",
        "            name=f\"flatten_confidence_{outputName}\", input_name=f\"{outputName}_raw_confidence\", output_name=f\"{outputName}_flatten_raw_confidence\", axis=-1)\n",
        "        builder.add_flatten_to_2d(\n",
        "            name=f\"flatten_coordinates_{outputName}\", input_name=f\"{outputName}_raw_normalized_coordinates\", output_name=f\"{outputName}_flatten_raw_coordinates\", axis=-1)\n",
        "\n",
        "    builder.add_concat_nd(name=\"concat_confidence\", input_names=[\n",
        "                          f\"{outputName}_flatten_raw_confidence\" for outputName in outputNames], output_name=\"raw_confidence\", axis=-2)\n",
        "    builder.add_concat_nd(name=\"concat_coordinates\", input_names=[\n",
        "                          f\"{outputName}_flatten_raw_coordinates\" for outputName in outputNames], output_name=\"raw_coordinates\", axis=-2)\n",
        "\n",
        "    builder.set_output(output_names=[\"raw_confidence\", \"raw_coordinates\"], output_dims=[\n",
        "                       (25200, numberOfClassLabels), (25200, 4)])\n",
        "\n",
        "\n",
        "def createNmsModelSpec(nnSpec):\n",
        "    '''\n",
        "    Create a coreml model with nms to filter the results of the model\n",
        "    '''\n",
        "    nmsSpec = ct.proto.Model_pb2.Model()\n",
        "    nmsSpec.specificationVersion = 4\n",
        "\n",
        "    # Define input and outputs of the model\n",
        "    for i in range(2):\n",
        "        nnOutput = nnSpec.description.output[i].SerializeToString()\n",
        "\n",
        "        nmsSpec.description.input.add()\n",
        "        nmsSpec.description.input[i].ParseFromString(nnOutput)\n",
        "\n",
        "        nmsSpec.description.output.add()\n",
        "        nmsSpec.description.output[i].ParseFromString(nnOutput)\n",
        "\n",
        "    nmsSpec.description.output[0].name = \"confidence\"\n",
        "    nmsSpec.description.output[1].name = \"coordinates\"\n",
        "\n",
        "    # Define output shape of the model\n",
        "    outputSizes = [numberOfClassLabels, 4]\n",
        "    for i in range(len(outputSizes)):\n",
        "        maType = nmsSpec.description.output[i].type.multiArrayType\n",
        "        # First dimension of both output is the number of boxes, which should be flexible\n",
        "        maType.shapeRange.sizeRanges.add()\n",
        "        maType.shapeRange.sizeRanges[0].lowerBound = 0\n",
        "        maType.shapeRange.sizeRanges[0].upperBound = -1\n",
        "        # Second dimension is fixed, for \"confidence\" it's the number of classes, for coordinates it's position (x, y) and size (w, h)\n",
        "        maType.shapeRange.sizeRanges.add()\n",
        "        maType.shapeRange.sizeRanges[1].lowerBound = outputSizes[i]\n",
        "        maType.shapeRange.sizeRanges[1].upperBound = outputSizes[i]\n",
        "        del maType.shape[:]\n",
        "\n",
        "    # Define the model type non maximum supression\n",
        "    nms = nmsSpec.nonMaximumSuppression\n",
        "    nms.confidenceInputFeatureName = \"raw_confidence\"\n",
        "    nms.coordinatesInputFeatureName = \"raw_coordinates\"\n",
        "    nms.confidenceOutputFeatureName = \"confidence\"\n",
        "    nms.coordinatesOutputFeatureName = \"coordinates\"\n",
        "    nms.iouThresholdInputFeatureName = \"iouThreshold\"\n",
        "    nms.confidenceThresholdInputFeatureName = \"confidenceThreshold\"\n",
        "    # Some good default values for the two additional inputs, can be overwritten when using the model\n",
        "    nms.iouThreshold = 0.6\n",
        "    nms.confidenceThreshold = 0.4\n",
        "    nms.stringClassLabels.vector.extend(classLabels)\n",
        "\n",
        "    return nmsSpec\n",
        "\n",
        "\n",
        "def combineModelsAndExport(builderSpec, nmsSpec, fileName, quantize=False):\n",
        "    '''\n",
        "    Combines the coreml model with export logic and the nms to one final model. Optionally save with different quantization (32, 16, 8) (Works only if on Mac Os)\n",
        "    '''\n",
        "    try:\n",
        "        print(f'Combine CoreMl model with nms and export model')\n",
        "        # Combine models to a single one\n",
        "        pipeline = ct.models.pipeline.Pipeline(input_features=[(\"image\", ct.models.datatypes.Array(3, 460, 460)),\n",
        "                                                               (\"iouThreshold\", ct.models.datatypes.Double(\n",
        "                                                               )),\n",
        "                                                               (\"confidenceThreshold\", ct.models.datatypes.Double())], output_features=[\"confidence\", \"coordinates\"])\n",
        "\n",
        "        # Required version (>= ios13) in order for mns to work\n",
        "        pipeline.spec.specificationVersion = 4\n",
        "\n",
        "        pipeline.add_model(builderSpec)\n",
        "        pipeline.add_model(nmsSpec)\n",
        "\n",
        "        pipeline.spec.description.input[0].ParseFromString(\n",
        "            builderSpec.description.input[0].SerializeToString())\n",
        "        pipeline.spec.description.output[0].ParseFromString(\n",
        "            nmsSpec.description.output[0].SerializeToString())\n",
        "        pipeline.spec.description.output[1].ParseFromString(\n",
        "            nmsSpec.description.output[1].SerializeToString())\n",
        "\n",
        "        # Metadata for the model‚\n",
        "        pipeline.spec.description.input[\n",
        "            1].shortDescription = \"(optional) IOU Threshold override (Default: 0.6)\"\n",
        "        pipeline.spec.description.input[\n",
        "            2].shortDescription = \"(optional) Confidence Threshold override (Default: 0.4)\"\n",
        "        pipeline.spec.description.output[0].shortDescription = u\"Boxes \\xd7 Class confidence\"\n",
        "        pipeline.spec.description.output[\n",
        "            1].shortDescription = u\"Boxes \\xd7 [x, y, width, height] (relative to image size)\"\n",
        "        pipeline.spec.description.metadata.versionString = \"1.0\"\n",
        "        pipeline.spec.description.metadata.shortDescription = \"yolov5\"\n",
        "        pipeline.spec.description.metadata.author = \"Leon De Andrade\"\n",
        "        pipeline.spec.description.metadata.license = \"\"\n",
        "\n",
        "        model = ct.models.MLModel(pipeline.spec)\n",
        "        model.save(fileName)\n",
        "\n",
        "        if quantize:\n",
        "            fileName16 = fileName.replace(\".mlmodel\", \"_16.mlmodel\")\n",
        "            modelFp16 = ct.models.neural_network.quantization_utils.quantize_weights(\n",
        "                model, nbits=16)\n",
        "            modelFp16.save(fileName16)\n",
        "\n",
        "            fileName8 = fileName.replace(\".mlmodel\", \"_8.mlmodel\")\n",
        "            modelFp8 = ct.models.neural_network.quantization_utils.quantize_weights(\n",
        "                model, nbits=8)\n",
        "            modelFp8.save(fileName8)\n",
        "\n",
        "        print(f'CoreML export success, saved as {fileName}')\n",
        "    except Exception as e:\n",
        "        print(f'CoreML export failure: {e}')\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument('--model-input-path', type=str, dest=\"model_input_path\",\n",
        "                        default='models/yolov5s_v4.pt', help='path to yolov5 model')\n",
        "    parser.add_argument('--model-output-directory', type=str,\n",
        "                        dest=\"model_output_directory\", default='output/models', help='model output path')\n",
        "    parser.add_argument('--model-output-name', type=str, dest=\"model_output_name\",\n",
        "                        default='yolov5-iOS', help='model output name')\n",
        "    parser.add_argument('--quantize-model', action=\"store_true\", dest=\"quantize\",\n",
        "                        help='Pass flag quantized models are needed (Only works on mac Os)')\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    if not Path(opt.model_input_path).exists():\n",
        "        print(\"Error: Input model not found\")\n",
        "        return\n",
        "\n",
        "    Path(opt.model_output_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    sampleInput = torch.zeros((1, 3, 640, 640))\n",
        "    checkInputs = [(torch.rand(1, 3, 640, 640),),\n",
        "                   (torch.rand(1, 3, 640, 640),)]\n",
        "\n",
        "    model = torch.load(opt.model_input_path, map_location=torch.device('cpu'))[\n",
        "        'model'].float()\n",
        "\n",
        "    model.train()   #eval→trainに修正\n",
        "    model.model[-1].export = True\n",
        "    # Dry run, necessary for correct tracing!\n",
        "    model(sampleInput)\n",
        "\n",
        "    ts = exportTorchscript(model, sampleInput, checkInputs,\n",
        "                           f\"{opt.model_output_directory}/{opt.model_output_name}.torchscript.pt\")\n",
        "\n",
        "    # Convert pytorch to raw coreml model\n",
        "    modelSpec = convertToCoremlSpec(ts, sampleInput)\n",
        "    addOutputMetaData(modelSpec)\n",
        "\n",
        "    # Add export logic to coreml model\n",
        "    builder = ct.models.neural_network.NeuralNetworkBuilder(spec=modelSpec)\n",
        "    addExportLayerToCoreml(builder)\n",
        "\n",
        "    # Create nms logic\n",
        "    nmsSpec = createNmsModelSpec(builder.spec)\n",
        "\n",
        "    # Combine model with export logic and nms logic\n",
        "    combineModelsAndExport(\n",
        "        builder.spec, nmsSpec, f\"{opt.model_output_directory}/{opt.model_output_name}.mlmodel\", opt.quantize)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\"\"\"\n",
        "path_w = 'main.py'\n",
        "\n",
        "with open(path_w, mode='w') as f:\n",
        "    f.write(text)\n"
      ],
      "metadata": {
        "id": "NdnBBcJtnd7_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q --pre poetry\n",
        "!pip3 install poetry\n",
        "!poetry --version"
      ],
      "metadata": {
        "id": "5T6lCmL7stfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/yolov5-coreml-tools\")\n",
        "!poetry install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcGI4a6Q8VRJ",
        "outputId": "5c0b6620-5cba-48a3-bf94-6e0d5370c909"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34mInstalling dependencies from lock file\u001b[39m\n",
            "\n",
            "No dependencies to install or update\n",
            "\n",
            "\u001b[39;1mInstalling\u001b[39;22m the current project: \u001b[36mcoreml-tools\u001b[39m (\u001b[39;1m0.2.1\u001b[39;22m)\u001b[1G\u001b[2K\u001b[39;1mInstalling\u001b[39;22m the current project: \u001b[36mcoreml-tools\u001b[39m (\u001b[32m0.2.1\u001b[39m)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!poetry run coreml-export "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOGxYK4R8Xfz",
        "outputId": "73ed8183-f660-433d-eeb6-2acc8605272b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/content/yolov5-coreml-tools/src/coreml_export/main.py\", line 313, in main\n",
            "    model = torch.load(opt.model_input_path, map_location=torch.device('cpu'))[\n",
            "  File \"/root/.cache/pypoetry/virtualenvs/coreml-tools-Wi__XBdL-py3.7/lib/python3.7/site-packages/torch/serialization.py\", line 594, in load\n",
            "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/root/.cache/pypoetry/virtualenvs/coreml-tools-Wi__XBdL-py3.7/lib/python3.7/site-packages/torch/serialization.py\", line 853, in _load\n",
            "    result = unpickler.load()\n",
            "  File \"/content/yolov5/models/yolo.py\", line 24, in <module>\n",
            "    from models.common import *\n",
            "  File \"/content/yolov5/models/common.py\", line 24, in <module>\n",
            "    from IPython.display import display\n",
            "ModuleNotFoundError: No module named 'IPython'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aUM0cE4DCs1s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}